{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtQn-fLh8lsi",
        "outputId": "ad759fb0-ed86-4877-fb5f-0ed8e02dcca6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pycytominer\n",
            "  Downloading pycytominer-1.3.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.12/dist-packages (from pycytominer) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from pycytominer) (2.2.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from pycytominer) (21.0.0)\n",
            "Requirement already satisfied: scikit-learn>=0.21.2 in /usr/local/lib/python3.12/dist-packages (from pycytominer) (1.6.1)\n",
            "Requirement already satisfied: scipy>1.13.1 in /usr/local/lib/python3.12/dist-packages (from pycytominer) (1.16.2)\n",
            "Collecting sqlalchemy<3,>=1.3.6 (from pycytominer)\n",
            "  Downloading sqlalchemy-2.0.43-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2.0->pycytominer) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2.0->pycytominer) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2.0->pycytominer) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.2->pycytominer) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.2->pycytominer) (3.6.0)\n",
            "Collecting greenlet>=1 (from sqlalchemy<3,>=1.3.6->pycytominer)\n",
            "  Downloading greenlet-3.2.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy<3,>=1.3.6->pycytominer) (4.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->pycytominer) (1.17.0)\n",
            "Downloading pycytominer-1.3.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sqlalchemy-2.0.43-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading greenlet-3.2.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (607 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m607.6/607.6 kB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: greenlet, sqlalchemy, pycytominer\n",
            "Successfully installed greenlet-3.2.4 pycytominer-1.3.0 sqlalchemy-2.0.43\n"
          ]
        }
      ],
      "source": [
        "!pip install pycytominer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1wWZWVRMAlL",
        "outputId": "7c4b47f7-c464-44a8-b527-6adc66a7676d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
            "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow)\n",
            "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: opt_einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf>=5.28.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (6.33.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing_extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.75.1)\n",
            "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras>=3.10.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
            "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (14.2.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard~=2.20.0->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (12.0.0)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard~=2.20.0->tensorflow)\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
            "Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.7/620.7 MB\u001b[0m \u001b[31m828.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m111.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m153.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m129.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: libclang, flatbuffers, wheel, werkzeug, tensorboard-data-server, google_pasta, tensorboard, astunparse, tensorflow\n",
            "Successfully installed astunparse-1.6.3 flatbuffers-25.9.23 google_pasta-0.2.0 libclang-18.1.1 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 werkzeug-3.1.3 wheel-0.45.1\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJMhopRvRa9j",
        "outputId": "b535fb4d-d92f-46c7-83d6-663c9daa68b9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jax/_src/cloud_tpu_init.py:86: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pycytominer import feature_select\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.model_selection import train_test_split,KFold\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, accuracy_score, RocCurveDisplay\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Dense, Dropout, Flatten,\n",
        "    Activation, BatchNormalization, ReLU,\n",
        "    Conv1D, Conv2D, AveragePooling1D, MaxPooling1D,GlobalAveragePooling1D,Add, TimeDistributed\n",
        ")\n",
        "from tensorflow.keras.optimizers import Adam, AdamW\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "from sklearn.ensemble import RandomForestClassifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvV2Egja8nYD",
        "outputId": "7bf5cd04-be56-4d85-e091-34b7f8d2f0b5"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'train.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2021400859.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train colonne\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"
          ]
        }
      ],
      "source": [
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "print(\"train :\", train.shape)\n",
        "print(\"test :\", test.shape)\n",
        "print(\"train colonne\", train.columns)\n",
        "print(\"test colonne\", test.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "thlE8WKzDM38"
      },
      "outputs": [],
      "source": [
        "train.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtuzV4SM-Mec"
      },
      "source": [
        "#Visualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NL9yCPua-PBs"
      },
      "outputs": [],
      "source": [
        "sns.countplot(x='Activity',data=train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Obvap12pF26q"
      },
      "outputs": [],
      "source": [
        "colonnes = list(train.columns)\n",
        "colonnes = colonnes[1:]\n",
        "print(colonnes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOMWmEnBQr9P"
      },
      "source": [
        "#Sélection des features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6eAKN7dgKtfj"
      },
      "outputs": [],
      "source": [
        "train[colonnes].corr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vAketDxhMY8m"
      },
      "outputs": [],
      "source": [
        "feature_select_ops = [\n",
        "    \"variance_threshold\",\n",
        "    \"correlation_threshold\",\n",
        "    \"drop_na_columns\",\n",
        "]\n",
        "na_cut = 0.05\n",
        "corr_threshold = 0.95\n",
        "output_dir = \"profiles\"\n",
        "\n",
        "profile_df = feature_select(\n",
        "                profiles=train,#donner nom df entier\n",
        "                operation=feature_select_ops,\n",
        "                features=colonnes,  # liste des features\n",
        "                na_cutoff=na_cut,\n",
        "                corr_threshold=corr_threshold\n",
        "            )\n",
        "selected_features = list(set(colonnes) & set(profile_df.columns))\n",
        "print(selected_features)\n",
        "print(len(selected_features))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Wtocc89wSLmR"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(train[selected_features].corr(), cmap=\"coolwarm\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y15TdpOoMQMJ"
      },
      "source": [
        "#Normalisation des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SmyazP1DMXXu"
      },
      "outputs": [],
      "source": [
        "test_normalize = normalize(test[selected_features], norm='l2')\n",
        "test_normalize = pd.DataFrame(test_normalize, columns=selected_features,index=test.index)\n",
        "train_normalize=normalize(train[selected_features], norm='l2')\n",
        "train_normalize = pd.DataFrame(train_normalize, columns=selected_features, index=train.index)\n",
        "train_normalize[\"Activity\"] = train[\"Activity\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PCI5_KkRGdvC"
      },
      "outputs": [],
      "source": [
        "test_normalize.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TlhQzVt3HRSd"
      },
      "outputs": [],
      "source": [
        "train_normalize.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29sjHR1WD2jn"
      },
      "source": [
        "#Visualisation T-sne"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Q2RlzxiODdfz"
      },
      "outputs": [],
      "source": [
        "m = TSNE(learning_rate=\"auto\")\n",
        "tsne_feature = m.fit_transform(train_normalize)\n",
        "train_normalize[\"x\"] = tsne_feature[:, 0]\n",
        "train_normalize[\"y\"] = tsne_feature[:, 1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gYzaOCaZE9lh"
      },
      "outputs": [],
      "source": [
        "sns.scatterplot(x=\"x\", y=\"y\", hue=\"Activity\",data=train_normalize)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWyBKEXPI5m4"
      },
      "source": [
        "Le graphique montre que les features sélectionnées permettent de bien discriminer les molécules en fonction de l'activité, on a deux groupes bien distinct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhvvDO-7K8rU"
      },
      "source": [
        "# Séparation train-test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JocKwLSGLIGK"
      },
      "outputs": [],
      "source": [
        "Y = train_normalize[\"Activity\"]\n",
        "X = train_normalize.drop([\"Activity\", \"x\", \"y\"], axis=1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zFAMph1fd38Y"
      },
      "outputs": [],
      "source": [
        "def specificity(y_true, y_pred):\n",
        "    y_pred = tf.round(y_pred)\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    tn = tf.reduce_sum((1 - y_true) * (1 - y_pred))\n",
        "    fp = tf.reduce_sum((1 - y_true) * y_pred)\n",
        "    return tn / (tn + fp + tf.keras.backend.epsilon())\n",
        "\n",
        "def f1_score(y_true, y_pred):\n",
        "    y_pred = tf.round(y_pred)\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    tp = tf.reduce_sum(y_true * y_pred)\n",
        "    fp = tf.reduce_sum((1 - y_true) * y_pred)\n",
        "    fn = tf.reduce_sum(y_true * (1 - y_pred))\n",
        "\n",
        "    precision = tp / (tp + fp + tf.keras.backend.epsilon())\n",
        "    recall = tp / (tp + fn + tf.keras.backend.epsilon())\n",
        "\n",
        "    return 2 * ((precision * recall) / (precision + recall + tf.keras.backend.epsilon()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDVt9Xeh3Rim"
      },
      "source": [
        "\n",
        "\n",
        "#Modèle dense\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Xcib4AXVhpRe"
      },
      "outputs": [],
      "source": [
        "def create_model_dense(input_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, activation='relu', input_shape=(input_dim,),\n",
        "                    kernel_regularizer=l2(0.001)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(64, activation='relu',\n",
        "                    kernel_regularizer=l2(0.001)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(32, activation='relu',\n",
        "                    kernel_regularizer=l2(0.001)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.0005),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=[\n",
        "            'accuracy',\n",
        "            Precision(name=\"precision\"),\n",
        "            Recall(name=\"recall\"),\n",
        "            specificity,\n",
        "            f1_score\n",
        "        ]\n",
        "    )\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_NuimZg5RlJ3"
      },
      "outputs": [],
      "source": [
        "model_dense = create_model_dense(X_train.shape[1])\n",
        "model_dense.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nfhmuEqo73QQ"
      },
      "outputs": [],
      "source": [
        "from keras.utils import plot_model\n",
        "\n",
        "plot_model(model_dense, to_file=\"model_dense.png\", show_shapes=True, show_layer_names=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "P-OWospp2LOw"
      },
      "outputs": [],
      "source": [
        "history_dense = model_dense.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test,y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mHBhCclM2k_e"
      },
      "outputs": [],
      "source": [
        "epochs = 100\n",
        "batch_size = 100\n",
        "n_splits = 5\n",
        "cv_accuracy = []\n",
        "cv_precision = []\n",
        "cv_recall = []\n",
        "cv_specificity = []\n",
        "cv_f1_score = []\n",
        "\n",
        "X_reset = X.reset_index(drop=True)\n",
        "y_reset = Y.reset_index(drop=True)\n",
        "\n",
        "kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kfold.split(X_reset)):\n",
        "    print(f\"\\n===== Fold {fold+1} =====\")\n",
        "\n",
        "    # Séparation des données\n",
        "    X_tr, X_val = X_reset.iloc[train_idx], X_reset.iloc[val_idx]\n",
        "    y_tr, y_val = y_reset.iloc[train_idx], y_reset.iloc[val_idx]\n",
        "    model = create_model_dense(X.shape[1])\n",
        "\n",
        "    # Entraînement\n",
        "    history_CV = model.fit(\n",
        "        X_tr, y_tr,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Évaluation sur le fold\n",
        "    scores_CV = model.evaluate(X_val, y_val, verbose=0)\n",
        "    loss, acc, prec, rec, spec, f1 = scores_CV\n",
        "    print(f\"Fold {fold+1} - Acc: {acc:.4f}, Prec: {prec:.4f}, Rec: {rec:.4f}, Spec: {spec:.4f}, F1: {f1:.4f}\") s\n",
        "    cv_accuracy.append(acc)\n",
        "    cv_precision.append(prec)\n",
        "    cv_recall.append(rec)\n",
        "    cv_specificity.append(spec)\n",
        "    cv_f1_score.append(f1)\n",
        "\n",
        "\n",
        "# Conserver l’historique du dernier fold\n",
        "history = history_CV\n",
        "# Résultats finaux\n",
        "print(\"\\n===== Résultats finaux =====\")\n",
        "print(\"Accuracy: %.2f%% (+/- %.2f%%)\" % (np.mean(cv_accuracy)*100, np.std(cv_accuracy)*100))\n",
        "print(\"Precision: %.2f%% (+/- %.2f%%)\" % (np.mean(cv_precision)*100, np.std(cv_precision)*100))\n",
        "print(\"Recall: %.2f%% (+/- %.2f%%)\" % (np.mean(cv_recall)*100, np.std(cv_recall)*100))\n",
        "print(\"Specificity: %.2f%% (+/- %.2f%%)\" % (np.mean(cv_specificity)*100, np.std(cv_specificity)*100))\n",
        "print(\"F1 Score: %.2f%% (+/- %.2f%%)\" % (np.mean(cv_f1_score)*100, np.std(cv_f1_score)*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IRIg9SdN3SA8"
      },
      "outputs": [],
      "source": [
        "#  \"Accuracy\"\n",
        "plt.plot(history_dense.history['accuracy'])\n",
        "plt.plot(history_dense.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "# \"Loss\"\n",
        "plt.plot(history_dense.history['loss'])\n",
        "plt.plot(history_dense.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-qMmhaneWlc"
      },
      "source": [
        "#Modèle convolution 1D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zdJ_ennxZhAR"
      },
      "outputs": [],
      "source": [
        "def create_conv1d(input_dim):\n",
        "    model = Sequential()\n",
        "    # Bloc 1\n",
        "    model.add(Conv1D(filters=64, kernel_size=7, padding='same', activation='relu',\n",
        "                     input_shape=(input_dim, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Dropout(0.25))\n",
        "    # Bloc 2\n",
        "    model.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Dropout(0.3))\n",
        "    # Bloc 3\n",
        "    model.add(Conv1D(filters=256, kernel_size=3, padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.3))\n",
        "    # Global pooling\n",
        "    model.add(GlobalAveragePooling1D())\n",
        "    # Dense layers\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.3))\n",
        "    # Sortie binaire\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    # Compilation\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.0005),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=[\n",
        "            'accuracy',\n",
        "            Precision(name=\"precision\"),\n",
        "            Recall(name=\"recall\"),\n",
        "            specificity,\n",
        "            f1_score\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-aa4O41lH4-n"
      },
      "outputs": [],
      "source": [
        "model_conv1D = create_conv1d(X_train.shape[1])\n",
        "model_conv1D.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "k2YJpP9u9ZYD"
      },
      "outputs": [],
      "source": [
        "history_conv1D = model_conv1D.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test,y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MkqrG3pbNEpX"
      },
      "outputs": [],
      "source": [
        "epochs = 100\n",
        "batch_size = 100\n",
        "n_splits = 5\n",
        "# Listes pour stocker les métriques par fold\n",
        "cv_accuracy = []\n",
        "cv_precision = []\n",
        "cv_recall = []\n",
        "cv_specificity = []\n",
        "cv_f1_score = []\n",
        "\n",
        "X_reset = X.reset_index(drop=True)\n",
        "y_reset = Y.reset_index(drop=True)\n",
        "\n",
        "kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kfold.split(X_reset)):\n",
        "    print(f\"\\n===== Fold {fold+1} =====\")\n",
        "\n",
        "    # Séparation des données\n",
        "    X_tr, X_val = X_reset.iloc[train_idx], X_reset.iloc[val_idx]\n",
        "    y_tr, y_val = y_reset.iloc[train_idx], y_reset.iloc[val_idx]\n",
        "    model = create_conv1d(X.shape[1])\n",
        "\n",
        "    # Entraînement\n",
        "    history_CV = model.fit(\n",
        "        X_tr, y_tr,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Évaluation sur le fold\n",
        "    scores_CV = model.evaluate(X_val, y_val, verbose=0)\n",
        "    loss, acc, prec, rec, spec, f1 = scores_CV\n",
        "    print(f\"Fold {fold+1} - Acc: {acc:.4f}, Prec: {prec:.4f}, Rec: {rec:.4f}, Spec: {spec:.4f}, F1: {f1:.4f}\") # Print all metrics\n",
        "    cv_accuracy.append(acc)\n",
        "    cv_precision.append(prec)\n",
        "    cv_recall.append(rec)\n",
        "    cv_specificity.append(spec)\n",
        "    cv_f1_score.append(f1)\n",
        "\n",
        "\n",
        "# Conserver l’historique du dernier fold\n",
        "history = history_CV\n",
        "# Résultats finaux\n",
        "print(\"\\n===== Résultats finaux =====\")\n",
        "print(\"Accuracy: %.2f%% (+/- %.2f%%)\" % (np.mean(cv_accuracy)*100, np.std(cv_accuracy)*100))\n",
        "print(\"Precision: %.2f%% (+/- %.2f%%)\" % (np.mean(cv_precision)*100, np.std(cv_precision)*100))\n",
        "print(\"Recall: %.2f%% (+/- %.2f%%)\" % (np.mean(cv_recall)*100, np.std(cv_recall)*100))\n",
        "print(\"Specificity: %.2f%% (+/- %.2f%%)\" % (np.mean(cv_specificity)*100, np.std(cv_specificity)*100)) # Print specificity\n",
        "print(\"F1 Score: %.2f%% (+/- %.2f%%)\" % (np.mean(cv_f1_score)*100, np.std(cv_f1_score)*100)) # Print f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gPFJiFasi6qz"
      },
      "outputs": [],
      "source": [
        "#  \"Accuracy\"\n",
        "plt.plot(history_conv1D.history['accuracy'])\n",
        "plt.plot(history_conv1D.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "# \"Loss\"\n",
        "plt.plot(history_conv1D.history['loss'])\n",
        "plt.plot(history_conv1D.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5fI0rqpk5OA"
      },
      "source": [
        "#Evaluations modèles regloss conv1D\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-31NffsEk7t7"
      },
      "outputs": [],
      "source": [
        "benchmark = pd.read_csv(\"svm_benchmark.csv\")\n",
        "benchmark.head()\n",
        "print(benchmark.shape)\n",
        "print(\"test :\", test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "E5nYlt3a3f9h"
      },
      "outputs": [],
      "source": [
        "predict_regloss = model_dense.predict(test_normalize)\n",
        "pred_labels = (predict_regloss > 0.5).astype(int)\n",
        "benchmark['TrueLabel'] = (benchmark['PredictedProbability'] > 0.5).astype(int)\n",
        "y_pred = pred_labels\n",
        "y_true = benchmark['TrueLabel']\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
        "disp.plot(cmap='Reds')\n",
        "plt.show()\n",
        "#metrique\n",
        "TN, FP, FN, TP = cm.ravel()\n",
        "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "sensitivity = TP / (TP + FN)\n",
        "specificity = TN / (TN + FP)\n",
        "# precision = recall\n",
        "precision = TP / (TP + FP)\n",
        "F1_score = (2*TP)/ ((2*TP) + FP + FN)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Sensitivity: {sensitivity:.4f}\")\n",
        "print(f\"Specificity: {specificity:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"F1 Score {F1_score:.4f}\")\n",
        "#Courbe ROC\n",
        "y_score = predict_regloss.ravel()\n",
        "roc_display = RocCurveDisplay.from_predictions(y_true, y_score)\n",
        "plt.plot([0,1],[0,1],'k--')\n",
        "plt.show()\n",
        "\n",
        "#AUC\n",
        "auc = roc_auc_score(y_true, y_score)\n",
        "print(f\"AUC: {auc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FToezGl0Uh9H"
      },
      "outputs": [],
      "source": [
        "predict_conv1D = model_conv1D.predict(test_normalize)\n",
        "pred_labels_conv1D = (predict_conv1D > 0.5).astype(int)\n",
        "benchmark['TrueLabel'] = (benchmark['PredictedProbability'] > 0.5).astype(int)\n",
        "y_pred_conv1D = pred_labels_conv1D\n",
        "y_true = benchmark['TrueLabel']\n",
        "cm_conv1D = confusion_matrix(y_true, y_pred_conv1D)\n",
        "disp_conv1D = ConfusionMatrixDisplay(confusion_matrix=cm_conv1D, display_labels=[0, 1])\n",
        "disp_conv1D.plot(cmap='Reds')\n",
        "plt.show()\n",
        "\n",
        "TN, FP, FN, TP = cm_conv1D.ravel()\n",
        "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "sensitivity = TP / (TP + FN)\n",
        "specificity = TN / (TN + FP)\n",
        "# precision = recall\n",
        "precision = TP / (TP + FP)\n",
        "F1_score = (2*TP)/ ((2*TP) + FP + FN)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Sensitivity: {sensitivity:.4f}\")\n",
        "print(f\"Specificity: {specificity:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"F1 Score {F1_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rttvFs463fSV"
      },
      "source": [
        "#DNN, 2nd learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XGb9lFyW3cXQ"
      },
      "outputs": [],
      "source": [
        "epoch = 20\n",
        "batch_size = 50\n",
        "learning = 0.001\n",
        "dropout = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BwYvf0NJ3hfb"
      },
      "outputs": [],
      "source": [
        "def dnn_model(learning, dropout, input_dim):\n",
        "    model = keras.Sequential([\n",
        "        layers.Input(shape=(input_dim,)),\n",
        "        layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(1e-4)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(dropout),\n",
        "\n",
        "        layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(1e-4)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(dropout),\n",
        "\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=learning)\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JDNKepTQ3sKi"
      },
      "outputs": [],
      "source": [
        "model_dnn = dnn_model(learning, dropout,X_train.shape[1])\n",
        "model_dnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QNXF_57GxkKX"
      },
      "outputs": [],
      "source": [
        "history_dnn = model_dnn.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test,y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "k2ppUrLVyXyv"
      },
      "outputs": [],
      "source": [
        "epochs = 100\n",
        "batch_size = 100\n",
        "n_splits = 5\n",
        "cv_scores = []\n",
        "\n",
        "X_reset = X.reset_index(drop=True)\n",
        "y_reset = Y.reset_index(drop=True)\n",
        "\n",
        "kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kfold.split(X_reset)):\n",
        "    print(f\"\\n===== Fold {fold+1} =====\")\n",
        "\n",
        "    # Séparation des données\n",
        "    X_tr, X_val = X_reset.iloc[train_idx], X_reset.iloc[val_idx]\n",
        "    y_tr, y_val = y_reset.iloc[train_idx], y_reset.iloc[val_idx]\n",
        "\n",
        "    # Recréer le modèle à chaque fold\n",
        "    model = dnn_model(learning,dropout,X.shape[1])\n",
        "\n",
        "    # Entraînement\n",
        "    history_CV = model.fit(\n",
        "        X_tr, y_tr,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Évaluation sur le fold\n",
        "    scores_CV = model.evaluate(X_val, y_val, verbose=0)\n",
        "    acc = scores_CV[1]\n",
        "    print(f\"Fold {fold+1} Accuracy: {acc:.4f}\")\n",
        "\n",
        "    cv_scores.append(acc)\n",
        "\n",
        "# Conserver l’historique du dernier fold (utile pour tracer les courbes)\n",
        "history = history_CV\n",
        "\n",
        "# Résultats finaux\n",
        "print(\"\\n===== Résultats finaux =====\")\n",
        "print(\"CV Accuracy: %.2f%% (+/- %.2f%%)\" % (np.mean(cv_scores)*100, np.std(cv_scores)*100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "O3FyIvkM4AiL"
      },
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Accuracy\n",
        "ax1.plot(history_dnn.history['accuracy'], label='Train Accuracy')\n",
        "ax1.plot(history_dnn.history['val_accuracy'], label='Val Accuracy')\n",
        "ax1.set_title('Accuracy')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.legend()\n",
        "\n",
        "# Loss\n",
        "ax2.plot(history_dnn.history['loss'], label='Train Loss')\n",
        "ax2.plot(history_dnn.history['val_loss'], label='Val Loss')\n",
        "ax2.set_title('Loss')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
